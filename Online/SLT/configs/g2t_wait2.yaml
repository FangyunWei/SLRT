task: G2T
data:
  dataset_name: phoenix
  dev: ../CSLR/results/online_slr/prediction_slide/phoenix14t_pred.dev
  test: ../CSLR/results/online_slr/prediction_slide/phoenix14t_pred.test
  train: ../../data/phoenix_2014t/phoenix14t.train
  input_data: gloss
  level: word
  max_sent_length: 400
  txt_lowercase: true
  transform_cfg:
    img_size: 224
    color_jitter: true
    bottom_area: 0.7 
    center_crop_size: 224 
    center_crop: False
    randomcrop_threshold: 1
    aspect_ratio_min: 0.75
    aspect_ratio_max: 1.3
    temporal_augmentation:
      tmin: 0.5
      tmax: 1.5
testing:
  cfg:
    translation:
      length_penalty: 1
      max_length: 100
      num_beams: 5
training:
  overwrite: False
  from_ckpt: True
  batch_size: 8  #8
  keep_last_ckpts: 1
  model_dir: results/g2t_wait2
  num_workers: 4
  optimization:
    betas:
    - 0.9
    - 0.998
    learning_rate:
      default: 1.0e-05
    optimizer: Adam
    scheduler: cosineannealing
    t_max: 80
    weight_decay: 0.001
  overwrite: true
  random_seed: 0
  shuffle: true
  total_epoch: 80
  validation:
    cfg:
      translation:
        length_penalty: 1
        max_length: 100
        num_beams: 5
    freq: 1
    unit: epoch
model:
  TranslationNetwork:
    GlossEmbedding:
      freeze: true
      gloss2embed_file: ../../pretrained_models/mBart_de/gloss_embeddings.bin
    GlossTokenizer:
      gloss2id_file: ../../pretrained_models/mBart_de/gloss2ids.pkl
      src_lang: de_DGS
    TextTokenizer:
      pretrained_model_name_or_path: ../../pretrained_models/mBart_de
      pruneids_file: ../../pretrained_models/mBart_de/map_ids.pkl
      tgt_lang: de_DE
    freeze_txt_embed: true
    label_smoothing: 0.2
    overwrite_cfg:
      attention_dropout: 0.1
      dropout: 0.3
      wait_k: 2
    pretrained_model_name_or_path: ../../pretrained_models/mBart_de