{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune mBART Embedding & Add Gloss Embeddings\n",
    "* CSL-Daily mBART_Zh\n",
    "* PHOENIX2014T mBART_De\n",
    "#### 1. Load original mBART with complete word embedding\n",
    "#### 2. Save special tokens first!\n",
    "#### 3. Gather vocab we need\n",
    "#### 4. Add subunits to embeddings\n",
    "#### 5. Save \n",
    "#### 6. Create gloss_embedding.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, gzip, pickle, json, numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import transformers\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load original mBART with complete word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size =  250027\n",
      "torch.Size([250027, 1024]) torch.Size([1, 250027])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MBartTokenizer.from_pretrained('../pretrained_models/mBart')\n",
    "model = MBartForConditionalGeneration.from_pretrained('../pretrained_models/mBart')\n",
    "print('Vocab size = ', tokenizer.vocab_size)\n",
    "full_embedding_weight = model.model.shared.weight\n",
    "full_final_logits_bias = model.final_logits_bias\n",
    "print(full_embedding_weight.shape, full_final_logits_bias.shape)\n",
    "with open(os.path.join('../pretrained_models/mBart/config.json'),'r') as f:\n",
    "    config_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save special tokens first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens # 30\n",
      "<s> 0->0\n",
      "<pad> 1->1\n",
      "</s> 2->2\n",
      "<unk> 3->3\n",
      "ar_AR 250001->4\n",
      "cs_CZ 250002->5\n",
      "de_DE 250003->6\n",
      "en_XX 250004->7\n",
      "es_XX 250005->8\n",
      "et_EE 250006->9\n",
      "fi_FI 250007->10\n",
      "fr_XX 250008->11\n",
      "gu_IN 250009->12\n",
      "hi_IN 250010->13\n",
      "it_IT 250011->14\n",
      "ja_XX 250012->15\n",
      "kk_KZ 250013->16\n",
      "ko_KR 250014->17\n",
      "lt_LT 250015->18\n",
      "lv_LV 250016->19\n",
      "my_MM 250017->20\n",
      "ne_NP 250018->21\n",
      "nl_XX 250019->22\n",
      "ro_RO 250020->23\n",
      "ru_RU 250021->24\n",
      "si_LK 250022->25\n",
      "tr_TR 250023->26\n",
      "vi_VN 250024->27\n",
      "zh_CN 250025->28\n",
      "<mask> 250026->29\n"
     ]
    }
   ],
   "source": [
    "new_embedding_weight_list = []\n",
    "new_final_logits_bias_list = []\n",
    "map_ids = {}\n",
    "with open(os.path.join('../pretrained_models/mBart/tokenizer.json'),'r') as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "print('Special tokens #', len(tokenizer_json['added_tokens']))\n",
    "for new_id, added_token in enumerate(tokenizer_json['added_tokens']):\n",
    "    id_ = added_token['id']\n",
    "    new_embedding_weight_list.append(full_embedding_weight[id_,:])\n",
    "    new_final_logits_bias_list.append(full_final_logits_bias[:,id_])\n",
    "    map_ids[id_] = new_id\n",
    "    print('{} {}->{}'.format(added_token['content'], id_, new_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gather vocab we need\n",
    "* sub-units for text (+gloss)\n",
    "* each gloss (lowercase) as a single unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_vocab(filename_format, tokenizer):\n",
    "    text_ids = defaultdict(int)\n",
    "    glosses = defaultdict(int)\n",
    "    for split in ['train','dev','test']:\n",
    "        with gzip.open(filename_format.format(split),'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for d in data:\n",
    "            input_ids = tokenizer(d['text'])['input_ids'][:-2]\n",
    "            for id_ in input_ids:\n",
    "                text_ids[id_] += 1\n",
    "            for gls in d['gloss'].lower().split():\n",
    "                input_ids = tokenizer(gls)['input_ids'][:-2]\n",
    "                for id_ in input_ids:\n",
    "                    text_ids[id_] += 1\n",
    "                    glosses[gls] += 1\n",
    "    print(os.path.dirname(filename_format), '#subunits=',len(text_ids), ' #gloss=',len(glosses))\n",
    "    return dict(text_ids), dict(glosses)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/PHOENIX #subunits= 2468  #gloss= 1115\n",
      "../data/CSL #subunits= 6357  #gloss= 2000\n"
     ]
    }
   ],
   "source": [
    "text2fre_de, gloss2fre_de = gather_vocab('../data/PHOENIX/phoenix14t.{}', tokenizer)\n",
    "text2fre_zh, gloss2fre_zh = gather_vocab('../data/CSL/csl.{}', tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add subunits to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of embedding list  30->2498\n",
      "Length of embedding list  30->3 already exists in embedding (a special token)\n",
      "6386\n"
     ]
    }
   ],
   "source": [
    "def add_subunit(subunits, \n",
    "    embedding_list, logits_list, map_ids):\n",
    "    offset = len(map_ids)\n",
    "    assert len(map_ids) == len(embedding_list)\n",
    "    print('Length of embedding list ', len(embedding_list),end='->')\n",
    "    for ii, sid in enumerate(subunits):\n",
    "        if sid in map_ids:\n",
    "            print(sid, 'already exists in embedding (a special token)')\n",
    "            continue\n",
    "        map_ids[sid] = len(embedding_list) #ii + offset \n",
    "        embedding_list.append(full_embedding_weight[sid,:])\n",
    "        logits_list.append(full_final_logits_bias[:,sid])\n",
    "    print(len(embedding_list))\n",
    "    assert len(map_ids)==len(embedding_list), (len(map_ids),len(embedding_list))\n",
    "    return embedding_list, logits_list, map_ids\n",
    "\n",
    "new_embedding_weight_list_de, new_final_logits_bias_list_de, map_ids_de = add_subunit(\n",
    "            text2fre_de, \n",
    "            new_embedding_weight_list[:], \n",
    "            new_final_logits_bias_list[:], deepcopy(map_ids))\n",
    "new_embedding_weight_list_zh, new_final_logits_bias_list_zh, map_ids_zh = add_subunit(\n",
    "            text2fre_zh, \n",
    "            new_embedding_weight_list[:], \n",
    "            new_final_logits_bias_list[:], deepcopy(map_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save \n",
    "* model_state_dict\n",
    "* config.json (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_logits_bias shape= torch.Size([1, 6386])\n",
      "new_embeddings shape= torch.Size([6386, 1024])\n",
      "new vocab size= 6386\n",
      "final_logits_bias shape= torch.Size([1, 2498])\n",
      "new_embeddings shape= torch.Size([2498, 1024])\n",
      "new vocab size= 2498\n"
     ]
    }
   ],
   "source": [
    "def save_new_model(src_dir, tgt_dir, new_logits_list, new_embeddings_list, map_ids):\n",
    "    os.makedirs(tgt_dir, exist_ok=True)\n",
    "    #1. cp tokenizer, sentencepiece\n",
    "    os.system('cp {} {}'.format(os.path.join(src_dir,'sentencepiece*'), tgt_dir))\n",
    "    os.system('cp {} {}'.format(os.path.join(src_dir,'tokenizer.json'), tgt_dir))\n",
    "    #2. model_state_dict\n",
    "    new_state_dict = deepcopy(model.state_dict())\n",
    "    new_state_dict['final_logits_bias'] = torch.cat(new_logits_list, dim=0).unsqueeze(0)\n",
    "    print('final_logits_bias shape=', new_state_dict['final_logits_bias'].shape)\n",
    "    new_state_dict['model.shared.weight'] = torch.stack(new_embeddings_list, dim=0)\n",
    "    print('new_embeddings shape=', new_state_dict['model.shared.weight'].shape)\n",
    "    new_state_dict['model.encoder.embed_tokens.weight'] = new_state_dict['model.shared.weight'] #model.encoder.embed_tokens.weight\n",
    "    new_state_dict['model.decoder.embed_tokens.weight'] = new_state_dict['model.shared.weight']\n",
    "    new_state_dict['lm_head.weight'] = new_state_dict['model.shared.weight']\n",
    "    torch.save(new_state_dict, os.path.join(tgt_dir, 'pytorch_model.bin'))\n",
    "    #3. config\n",
    "    new_config_json = deepcopy(config_json)\n",
    "    new_config_json['vocab_size'] = new_state_dict['model.shared.weight'].shape[0]\n",
    "    print('new vocab size=', new_config_json['vocab_size'])\n",
    "    with open(os.path.join(tgt_dir,'config.json'),'w') as f:\n",
    "        json.dump(new_config_json, f)\n",
    "    #4.map_ids:\n",
    "    assert len(map_ids) == new_config_json['vocab_size']\n",
    "    with open(os.path.join(tgt_dir,'map_ids.pkl'),'wb') as f:\n",
    "        pickle.dump(map_ids, f)\n",
    "\n",
    "save_new_model(\n",
    "    '../pretrained_models/mBart', '../pretrained_models/mBart_zh',\n",
    "    new_logits_list=new_final_logits_bias_list_zh, \n",
    "    new_embeddings_list=new_embedding_weight_list_zh, \n",
    "    map_ids=map_ids_zh)\n",
    "save_new_model(\n",
    "    '../pretrained_models/mBart', '../pretrained_models/mBart_de',\n",
    "    new_logits_list=new_final_logits_bias_list_de, \n",
    "    new_embeddings_list=new_embedding_weight_list_de, \n",
    "    map_ids=map_ids_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create gloss_embedding.bin and Save\n",
    "* For each gloss, we average embeddings of its subunits as the gloss embedding\n",
    "* </s> <lang> <unk> <mask>\n",
    "* zh_CSL, de_DGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token 0 <s>\n",
      "Special token 1 <pad>\n",
      "Special token 2 </s>\n",
      "Special token 3 <unk>\n",
      "Special token 250026 <mask>\n",
      "Special token 250025 zh_CN\n",
      "Special token 250003 de_DE\n",
      "2000 2009\n",
      "Special token 0 <s>\n",
      "Special token 1 <pad>\n",
      "Special token 2 </s>\n",
      "Special token 3 <unk>\n",
      "Special token 250026 <mask>\n",
      "Special token 250025 zh_CN\n",
      "Special token 250003 de_DE\n",
      "1115 1124\n"
     ]
    }
   ],
   "source": [
    "def create_gloss_embedding(glosses):\n",
    "    gls2emb = {}\n",
    "    #special tokens!\n",
    "    #</s><lang><unk><mask>\n",
    "    for t in ['<s>', '<pad>', '</s>', '<unk>','<mask>','zh_CN','de_DE']:\n",
    "        emb_id = tokenizer.convert_tokens_to_ids(t)\n",
    "        print('Special token {} {}'.format(emb_id, t))\n",
    "        gls2emb[t] = full_embedding_weight[emb_id,:]\n",
    "    gls2emb['zh_CSL'] = gls2emb['zh_CN']\n",
    "    gls2emb['de_DGS'] = gls2emb['de_DE']\n",
    "    #gls\n",
    "    for gls in glosses:\n",
    "        gls = gls.lower()\n",
    "        gls_ids = tokenizer(gls)['input_ids'][:-2] # remove</s> <lang>\n",
    "        emb = []\n",
    "        for i in gls_ids:\n",
    "            emb.append(full_embedding_weight[i,:])\n",
    "        emb = torch.mean(torch.stack(emb, dim=0), dim=0)\n",
    "        gls2emb[gls] = emb\n",
    "    print(len(glosses), len(gls2emb))\n",
    "    return gls2emb\n",
    "gls2emb_zh =  create_gloss_embedding(gloss2fre_zh)\n",
    "torch.save(gls2emb_zh, os.path.join('../pretrained_models/mBart_zh/gloss_embeddings.bin'))\n",
    "gls2emb_de =  create_gloss_embedding(gloss2fre_de)\n",
    "torch.save(gls2emb_de, os.path.join('../pretrained_models/mBart_de/gloss_embeddings.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gloss_index(gls2emb, output_dir):\n",
    "    gls2id = {}\n",
    "    for id_, gls in enumerate(gls2emb):\n",
    "        gls2id[gls] = id_\n",
    "    with open(os.path.join(output_dir,'gloss2ids.pkl'),'wb') as f:\n",
    "        pickle.dump(gls2id, f)\n",
    "    # print(len(gls2id))\n",
    "    # print(gls2id)\n",
    "save_gloss_index(gls2emb_zh, '../pretrained_models/mBart_zh')\n",
    "save_gloss_index(gls2emb_de, '../pretrained_models/mBart_de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a69c62d3f70bee50f2da89d7d3e360498661a2f4e4398723ca383c320ac4960b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('slt_update': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
